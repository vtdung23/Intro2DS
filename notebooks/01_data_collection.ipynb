{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae56774",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "\n",
    "## Prerequisites\n",
    "1. Python >= 3.9\n",
    "2. TMDB v3 API key\n",
    "3. Optional: `requests` library (will fallback to urllib if not available)\n",
    "\n",
    "## Setup Steps\n",
    "\n",
    "### 1. Get TMDB API Key\n",
    "- Register at https://www.themoviedb.org/\n",
    "- Go to Settings > API > Request API Key\n",
    "- Copy your API key (v3 auth)\n",
    "\n",
    "### 2. Set Environment Variable\n",
    "**Windows (PowerShell):**\n",
    "```powershell\n",
    "$env:TMDB_API_KEY = \"your_api_key_here\"\n",
    "```\n",
    "\n",
    "**Windows (CMD):**\n",
    "```cmd\n",
    "set TMDB_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "export TMDB_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "### 3. Install Dependencies (Optional)\n",
    "```bash\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "### 4. Run the Notebook\n",
    "1. Open this notebook in Jupyter or VS Code\n",
    "2. **Important**: Update the `TMDB_API_KEY` variable in Cell 2 with your actual API key\n",
    "3. Restart the kernel after setting the API key\n",
    "4. Run cells sequentially from top to bottom or run all\n",
    "5. For testing: Run Cell 6 (sample run with 150 movies)\n",
    "6. For full harvest: Uncomment and run Cell 7 (20,000+ movies)\n",
    "\n",
    "## Output Files\n",
    "- `movies_full.jsonl`: JSONL format checkpoint file\n",
    "- `movies_full.csv`: CSV format with all enriched movie data\n",
    "\n",
    "## Notes\n",
    "- Adjust `SUMMARY_TARGET`, `SUMMARY_START_YEAR` in Cell 2 to customize data collection\n",
    "- Increase delay parameters if you encounter rate limiting (HTTP 429)\n",
    "- The notebook supports resume from checkpoint - rerun to continue from where it stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac356aa8",
   "metadata": {},
   "source": [
    "# TMDB Movie Dataset Harvest Notebook\n",
    "\n",
    "This notebook helps you build a local movie dataset (summary + enriched metadata) from TMDB API in a maintainable, restartable way.\n",
    "\n",
    "Contents:\n",
    "1. Configuration & API Key\n",
    "2. Core HTTP utilities (retry, backoff)\n",
    "3. Summary collection via /discover (segmented by year)\n",
    "4. Detail enrichment (append_to_response blocks)\n",
    "5. Checkpointed harvesting (JSONL + CSV)\n",
    "6. Usage tips & performance notes\n",
    "\n",
    "Prerequisites:\n",
    "- TMDB v3 API key set as environment variable `TMDB_API_KEY`.\n",
    "- Python >= 3.9. Optional: `requests` library (will fallback to urllib).\n",
    "\n",
    "Set API key (PowerShell / Windows):\n",
    "```powershell\n",
    "$env:TMDB_API_KEY = \"YOUR_TMDB_API_KEY\"\n",
    "```\n",
    "Restart kernel after setting if it wasn't previously defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eac56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Configuration\n",
    "import os, json, time, csv, math, pathlib\n",
    "from typing import Dict, Any, List, Optional, Iterable, Sequence\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import requests  # type: ignore\n",
    "    HAVE_REQUESTS = True\n",
    "except Exception:\n",
    "    import urllib.request, urllib.parse\n",
    "    HAVE_REQUESTS = False\n",
    "TMDB_API_KEY = \"\"\n",
    "# TMDB_API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not TMDB_API_KEY:\n",
    "    raise RuntimeError(\"Missing TMDB_API_KEY. Set environment variable before proceeding.\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "LANGUAGE = \"en-US\"\n",
    "\n",
    "# Tuning parameters (adjust as needed)\n",
    "SUMMARY_TARGET = 20000         # number of summary rows desired\n",
    "SUMMARY_START_YEAR = 2000      # earliest release year\n",
    "SUMMARY_DELAY = 0.12           # delay between discover page requests\n",
    "DETAIL_APPEND_BLOCKS = [\n",
    "    \"credits\",\"keywords\",\"release_dates\",\"videos\",\"external_ids\"\n",
    "]\n",
    "DETAIL_RATE_DELAY = 0.12       # delay between detail requests\n",
    "DETAIL_BATCH_SIZE = 500        # flush interval for checkpoint files\n",
    "CHECKPOINT_PREFIX = \"movies_full\"  # base name for output files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Core HTTP utilities (retry, backoff)\n",
    "\n",
    "def http_get(path: str, params: Optional[Dict[str, Any]] = None,\n",
    "             retries: int = 3, backoff: float = 1.5, timeout: int = 30) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Thực hiện HTTP GET request đến TMDB API với retry và backoff logic.\n",
    "    \n",
    "    Input:\n",
    "        - path (str): Đường dẫn API endpoint (ví dụ: \"/movie/123\")\n",
    "        - params (Optional[Dict[str, Any]]): Dictionary các tham số query string\n",
    "        - retries (int): Số lần retry khi gặp lỗi (mặc định: 3)\n",
    "        - backoff (float): Hệ số backoff cho mỗi lần retry (mặc định: 1.5)\n",
    "        - timeout (int): Timeout cho request tính bằng giây (mặc định: 30)\n",
    "    \n",
    "    Output:\n",
    "        - Dict[str, Any]: JSON response từ API dưới dạng dictionary\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm gọi API TMDB với cơ chế retry tự động khi gặp lỗi. Tự động thêm api_key\n",
    "        vào params. Xử lý rate limiting (HTTP 429) bằng cách đợi theo Retry-After header.\n",
    "        Hỗ trợ cả thư viện requests và urllib (fallback).\n",
    "    \"\"\"\n",
    "    params = dict(params or {})\n",
    "    params[\"api_key\"] = TMDB_API_KEY\n",
    "\n",
    "    if HAVE_REQUESTS:\n",
    "        url = f\"{BASE_URL}{path}\"\n",
    "        last_exc: Optional[Exception] = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                resp = requests.get(url, params=params, timeout=timeout)\n",
    "                if resp.status_code == 429:\n",
    "                    wait = int(resp.headers.get(\"Retry-After\", 2))\n",
    "                    time.sleep(wait)\n",
    "                    continue\n",
    "                resp.raise_for_status()\n",
    "                return resp.json()\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(backoff ** attempt)\n",
    "        assert False, f\"Unreachable: {last_exc}\"\n",
    "    else:\n",
    "        qs = urllib.parse.urlencode(params)\n",
    "        url = f\"{BASE_URL}{path}?{qs}\"\n",
    "        last_exc: Optional[Exception] = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                with urllib.request.urlopen(url, timeout=timeout) as resp:\n",
    "                    status = resp.getcode()\n",
    "                    if status == 429:\n",
    "                        wait = int(resp.headers.get(\"Retry-After\", \"2\"))\n",
    "                        time.sleep(wait)\n",
    "                        continue\n",
    "                    if status >= 400:\n",
    "                        raise RuntimeError(f\"HTTP {status}: {url}\")\n",
    "                    data = resp.read()\n",
    "                    return json.loads(data.decode(\"utf-8\"))\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(backoff ** attempt)\n",
    "        assert False, f\"Unreachable: {last_exc}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d384797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Discover summary collection (segmented by year)\n",
    "\n",
    "def discover_movies(target_count: int,\n",
    "                    start_year: int,\n",
    "                    end_year: int,\n",
    "                    per_page_delay: float = 0.1,\n",
    "                    sort_by: str = \"popularity.desc\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Thu thập danh sách tóm tắt phim từ TMDB discover API, phân đoạn theo năm.\n",
    "    \n",
    "    Input:\n",
    "        - target_count (int): Số lượng phim mục tiêu cần thu thập\n",
    "        - start_year (int): Năm bắt đầu (ví dụ: 2000)\n",
    "        - end_year (int): Năm kết thúc (ví dụ: 2025)\n",
    "        - per_page_delay (float): Thời gian delay giữa các request (giây), mặc định: 0.1\n",
    "        - sort_by (str): Tiêu chí sắp xếp, mặc định: \"popularity.desc\"\n",
    "    \n",
    "    Output:\n",
    "        - List[Dict[str, Any]]: Danh sách các dictionary chứa thông tin tóm tắt phim\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm duyệt qua các năm từ start_year đến end_year, với mỗi năm gọi discover API\n",
    "        để lấy danh sách phim. Tự động phân trang và loại bỏ các phim trùng lặp.\n",
    "        Dừng khi đạt đủ target_count hoặc hết dữ liệu.\n",
    "    \"\"\"\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "    seen_ids = set()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if len(collected) >= target_count:\n",
    "            break\n",
    "        page = 1\n",
    "        while page <= 500 and len(collected) < target_count:\n",
    "            payload = http_get(\n",
    "                \"/discover/movie\",\n",
    "                {\n",
    "                    \"language\": LANGUAGE,\n",
    "                    \"sort_by\": sort_by,\n",
    "                    \"include_adult\": False,\n",
    "                    \"include_video\": False,\n",
    "                    \"primary_release_year\": year,\n",
    "                    \"page\": page,\n",
    "                },\n",
    "            )\n",
    "            results = payload.get(\"results\", [])\n",
    "            if not results:\n",
    "                break\n",
    "            for mv in results:\n",
    "                mid = mv.get(\"id\")\n",
    "                if mid is None or mid in seen_ids:\n",
    "                    continue\n",
    "                seen_ids.add(mid)\n",
    "                collected.append(mv)\n",
    "                if len(collected) >= target_count:\n",
    "                    break\n",
    "            page += 1\n",
    "            if per_page_delay:\n",
    "                time.sleep(per_page_delay)\n",
    "    return collected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Detail fetch + flatten\n",
    "\n",
    "def fetch_movie_detail(movie_id: int,\n",
    "                       append_blocks: Optional[Sequence[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Lấy thông tin chi tiết của một phim từ TMDB API.\n",
    "    \n",
    "    Input:\n",
    "        - movie_id (int): ID của phim trên TMDB\n",
    "        - append_blocks (Optional[Sequence[str]]): Danh sách các block dữ liệu bổ sung\n",
    "          (ví dụ: [\"credits\", \"keywords\", \"videos\"])\n",
    "    \n",
    "    Output:\n",
    "        - Dict[str, Any]: Dictionary chứa toàn bộ thông tin chi tiết của phim\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm gọi API endpoint /movie/{movie_id} để lấy thông tin đầy đủ của phim.\n",
    "        Sử dụng tham số append_to_response để lấy thêm các block dữ liệu như\n",
    "        credits, keywords, release_dates trong một request duy nhất.\n",
    "    \"\"\"\n",
    "    append = \",\".join(append_blocks or [])\n",
    "    params = {\"language\": LANGUAGE}\n",
    "    if append:\n",
    "        params[\"append_to_response\"] = append\n",
    "    return http_get(f\"/movie/{movie_id}\", params)\n",
    "\n",
    "\n",
    "def flatten_detail(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Chuyển đổi JSON chi tiết phim phức tạp thành dictionary phẳng cho CSV export.\n",
    "    \n",
    "    Input:\n",
    "        - d (Dict[str, Any]): Dictionary chứa dữ liệu chi tiết phim từ API\n",
    "    \n",
    "    Output:\n",
    "        - Dict[str, Any]: Dictionary đã được làm phẳng với các field đơn giản\n",
    "          và các list được join thành string\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm trích xuất và làm phẳng các trường quan trọng từ JSON response phức tạp.\n",
    "        Xử lý nested objects (collection, genres, companies, countries, credits, etc.).\n",
    "        Chuyển đổi các list thành chuỗi phân tách bằng dấu phẩy để dễ lưu vào CSV.\n",
    "        Trích xuất certification (US), trailer key, external IDs, top 5 cast và directors.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "    basic = [\"id\",\"title\",\"original_title\",\"overview\",\"release_date\",\"original_language\",\"imdb_id\",\"budget\",\"revenue\",\"runtime\",\"status\",\"tagline\",\"popularity\",\"vote_count\",\"vote_average\"]\n",
    "    for k in basic:\n",
    "        out[k] = d.get(k)\n",
    "    coll = d.get(\"belongs_to_collection\") or {}\n",
    "    out[\"collection_id\"] = coll.get(\"id\")\n",
    "    out[\"collection_name\"] = coll.get(\"name\")\n",
    "    out[\"genres\"] = \",\".join(g.get(\"name\") for g in d.get(\"genres\", []) if g.get(\"name\"))\n",
    "    out[\"production_companies\"] = \",\".join(pc.get(\"name\") for pc in d.get(\"production_companies\", []) if pc.get(\"name\"))\n",
    "    out[\"production_countries\"] = \",\".join(pc.get(\"iso_3166_1\") for pc in d.get(\"production_countries\", []) if pc.get(\"iso_3166_1\"))\n",
    "    out[\"spoken_languages\"] = \",\".join(sl.get(\"iso_639_1\") for sl in d.get(\"spoken_languages\", []) if sl.get(\"iso_639_1\"))\n",
    "    kws = d.get(\"keywords\", {}).get(\"keywords\", [])\n",
    "    out[\"keywords\"] = \",\".join(k.get(\"name\") for k in kws if k.get(\"name\"))\n",
    "    # Release certification (US)\n",
    "    cert = None\n",
    "    for country in d.get(\"release_dates\", {}).get(\"results\", []):\n",
    "        if country.get(\"iso_3166_1\") == \"US\":\n",
    "            rels = country.get(\"release_dates\", [])\n",
    "            if rels:\n",
    "                cert = rels[0].get(\"certification\")\n",
    "            break\n",
    "    out[\"certification_US\"] = cert\n",
    "    # Trailer key\n",
    "    vids = d.get(\"videos\", {}).get(\"results\", [])\n",
    "    trailer = next((v for v in vids if v.get(\"type\") == \"Trailer\"), None)\n",
    "    out[\"trailer_key\"] = trailer.get(\"key\") if trailer else None\n",
    "    # External IDs\n",
    "    ext = d.get(\"external_ids\", {})\n",
    "    for k in [\"facebook_id\",\"instagram_id\",\"twitter_id\"]:\n",
    "        out[k] = ext.get(k)\n",
    "    # Credits (top cast + directors)\n",
    "    cast = d.get(\"credits\", {}).get(\"cast\", [])\n",
    "    crew = d.get(\"credits\", {}).get(\"crew\", [])\n",
    "    out[\"cast_top5\"] = \",\".join(c.get(\"name\") for c in cast[:5] if c.get(\"name\"))\n",
    "    directors = [c.get(\"name\") for c in crew if c.get(\"job\") == \"Director\" and c.get(\"name\")]\n",
    "    out[\"directors\"] = \",\".join(directors)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Checkpointed enrichment pipeline\n",
    "CHECKPOINT_JSONL = pathlib.Path(f\"{CHECKPOINT_PREFIX}.jsonl\")\n",
    "CHECKPOINT_CSV = pathlib.Path(f\"{CHECKPOINT_PREFIX}.csv\")\n",
    "\n",
    "\n",
    "def load_processed_ids(jsonl_path: pathlib.Path) -> set:\n",
    "    \"\"\"\n",
    "    Đọc danh sách ID phim đã được xử lý từ file checkpoint JSONL.\n",
    "    \n",
    "    Input:\n",
    "        - jsonl_path (pathlib.Path): Đường dẫn đến file JSONL checkpoint\n",
    "    \n",
    "    Output:\n",
    "        - set: Set chứa các movie ID đã được xử lý\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm đọc file JSONL checkpoint và trích xuất các movie ID đã được xử lý.\n",
    "        Dùng để tránh xử lý trùng lặp khi resume từ checkpoint. Nếu file không\n",
    "        tồn tại, trả về set rỗng.\n",
    "    \"\"\"\n",
    "    ids = set()\n",
    "    if jsonl_path.exists():\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    mid = obj.get(\"id\")\n",
    "                    if mid is not None:\n",
    "                        ids.add(mid)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return ids\n",
    "\n",
    "\n",
    "def flush_rows(rows: List[Dict[str, Any]], csv_path: pathlib.Path, jsonl_path: pathlib.Path, field_order: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ghi buffer rows vào file checkpoint (JSONL và CSV).\n",
    "    \n",
    "    Input:\n",
    "        - rows (List[Dict[str, Any]]): List các dictionary chứa dữ liệu phim cần ghi\n",
    "        - csv_path (pathlib.Path): Đường dẫn file CSV output\n",
    "        - jsonl_path (pathlib.Path): Đường dẫn file JSONL output\n",
    "        - field_order (List[str]): Thứ tự các field cho CSV header\n",
    "    \n",
    "    Output:\n",
    "        - List[str]: Thứ tự field đã được cập nhật (hoặc giữ nguyên)\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm ghi batch data vào cả file JSONL (append) và CSV (append).\n",
    "        Tự động xác định field order ở lần ghi đầu tiên. Ghi CSV header\n",
    "        nếu file chưa tồn tại. Sau khi ghi xong, xóa buffer rows.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return field_order\n",
    "    # JSONL append\n",
    "    with open(jsonl_path, \"a\", encoding=\"utf-8\") as jf:\n",
    "        for r in rows:\n",
    "            jf.write(json.dumps(r, ensure_ascii=False))\n",
    "            jf.write(\"\\n\")\n",
    "    # Determine field order first time\n",
    "    if not field_order:\n",
    "        keys = set()\n",
    "        for r in rows:\n",
    "            keys.update(r.keys())\n",
    "        field_order = sorted(keys)\n",
    "    write_header = not csv_path.exists()\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as cf:\n",
    "        writer = csv.DictWriter(cf, fieldnames=field_order)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow({k: r.get(k) for k in field_order})\n",
    "    rows.clear()\n",
    "    return field_order\n",
    "\n",
    "\n",
    "def enrich_movies(summary: List[Dict[str, Any]],\n",
    "                  append_blocks: Sequence[str],\n",
    "                  batch_size: int = 500,\n",
    "                  rate_delay: float = 0.12) -> None:\n",
    "    \"\"\"\n",
    "    Làm giàu dữ liệu phim từ danh sách summary bằng cách fetch detail và ghi checkpoint.\n",
    "    \n",
    "    Input:\n",
    "        - summary (List[Dict[str, Any]]): Danh sách các dictionary chứa summary phim\n",
    "        - append_blocks (Sequence[str]): Các block dữ liệu bổ sung cần fetch\n",
    "          (ví dụ: [\"credits\", \"keywords\", \"release_dates\", \"videos\"])\n",
    "        - batch_size (int): Số lượng rows để flush checkpoint (mặc định: 500)\n",
    "        - rate_delay (float): Delay giữa các request detail (giây), mặc định: 0.12\n",
    "    \n",
    "    Output:\n",
    "        - None (ghi trực tiếp vào file checkpoint)\n",
    "    \n",
    "    Mô tả:\n",
    "        Hàm chính để làm giàu dữ liệu phim. Với mỗi phim trong summary:\n",
    "        1. Check nếu đã xử lý (từ checkpoint) thì bỏ qua\n",
    "        2. Fetch detail từ API\n",
    "        3. Flatten detail thành format phẳng\n",
    "        4. Thêm vào buffer và flush theo batch_size\n",
    "        5. Ghi checkpoint định kỳ để hỗ trợ resume\n",
    "        Hiển thị progress và thời gian xử lý khi hoàn thành.\n",
    "    \"\"\"\n",
    "    processed_ids = load_processed_ids(CHECKPOINT_JSONL)\n",
    "    buffer: List[Dict[str, Any]] = []\n",
    "    field_order: List[str] = []\n",
    "    start = time.time()\n",
    "    for idx, mv in enumerate(summary, 1):\n",
    "        mid = mv.get(\"id\")\n",
    "        if mid is None or mid in processed_ids:\n",
    "            continue\n",
    "        try:\n",
    "            detail = fetch_movie_detail(int(mid), append_blocks)\n",
    "            flat = flatten_detail(detail)\n",
    "            buffer.append(flat)\n",
    "            processed_ids.add(mid)\n",
    "        except Exception as e:\n",
    "            print(f\"Error {mid}: {e}\")\n",
    "        if len(buffer) >= batch_size:\n",
    "            field_order = flush_rows(buffer, CHECKPOINT_CSV, CHECKPOINT_JSONL, field_order)\n",
    "            print(f\"Checkpoint: {len(processed_ids)} enriched\")\n",
    "        if rate_delay:\n",
    "            time.sleep(rate_delay)\n",
    "    # final flush\n",
    "    if buffer:\n",
    "        field_order = flush_rows(buffer, CHECKPOINT_CSV, CHECKPOINT_JSONL, field_order)\n",
    "    duration = time.time() - start\n",
    "    print(f\"Done. Total enriched: {len(processed_ids)} | Time: {duration/60:.2f} min\")\n",
    "    print(f\"Files: {CHECKPOINT_JSONL}, {CHECKPOINT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89331cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample summary rows: 150\n",
      "Checkpoint: 200 enriched\n",
      "Checkpoint: 200 enriched\n",
      "Checkpoint: 250 enriched\n",
      "Checkpoint: 250 enriched\n",
      "Checkpoint: 300 enriched\n",
      "Done. Total enriched: 300 | Time: 2.04 min\n",
      "Files: movies_full.jsonl, movies_full.csv\n",
      "Checkpoint: 300 enriched\n",
      "Done. Total enriched: 300 | Time: 2.04 min\n",
      "Files: movies_full.jsonl, movies_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 6. Example: small sample run (safe quick test)\n",
    "# Uncomment to test with a small number before full harvest.\n",
    "SAMPLE_COUNT = 150\n",
    "sample_summary = discover_movies(\n",
    "    target_count=SAMPLE_COUNT,\n",
    "    start_year=2022,\n",
    "    end_year=datetime.now().year,\n",
    "    per_page_delay=0.05,\n",
    ")\n",
    "print(f\"Sample summary rows: {len(sample_summary)}\")\n",
    "enrich_movies(sample_summary, DETAIL_APPEND_BLOCKS, batch_size=50, rate_delay=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed1ad264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Full harvest (summary + enrichment)\n",
    "# Run when ready. Estimated time varies with network & rate limits.\n",
    "# WARNING: This may take several minutes for 20k + detail enrichment.\n",
    "# Uncomment the lines below to execute.\n",
    "\n",
    "# summary_movies = discover_movies(\n",
    "#     target_count=SUMMARY_TARGET,\n",
    "#     start_year=SUMMARY_START_YEAR,\n",
    "#     end_year=datetime.now().year,\n",
    "#     per_page_delay=SUMMARY_DELAY,\n",
    "# )\n",
    "# print(f\"Summary collected: {len(summary_movies)}\")\n",
    "# save_jsonl = lambda items, path: open(path, 'w', encoding='utf-8').write('\\n'.join(json.dumps(it, ensure_ascii=False) for it in items))\n",
    "# save_jsonl(summary_movies, 'movies_summary.jsonl')\n",
    "# print('Saved movies_summary.jsonl')\n",
    "# enrich_movies(summary_movies, DETAIL_APPEND_BLOCKS, batch_size=DETAIL_BATCH_SIZE, rate_delay=DETAIL_RATE_DELAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e07fb3",
   "metadata": {},
   "source": [
    "## 8. Usage & Performance Notes\n",
    "- Summary endpoints return limited fields; enrichment adds budget, revenue, runtime, companies, countries, languages, collection, external IDs, credits.\n",
    "- Adjust delays if you face HTTP 429 (rate limiting). Increase `SUMMARY_DELAY` / `DETAIL_RATE_DELAY`.\n",
    "- Add blocks: include `images,recommendations,similar` in `DETAIL_APPEND_BLOCKS` for more breadth (heavier & slower).\n",
    "- Resume support: rerunning enrichment will continue from existing JSONL/CSV checkpoint.\n",
    "- For experimentation, start with small sample (Cell 6) before full run (Cell 7).\n",
    "- Consider official TMDB bulk dataset (e.g. Kaggle mirrors) if you need more than 20k + faster turnaround.\n",
    "- Remove any API key from code cells before sharing or committing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
